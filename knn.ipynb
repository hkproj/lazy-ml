{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors algorithm\n",
    "\n",
    "As the name implies, the k-nearest neighbors algorithm works by findinng the nearest neighbors of some give data. For instance, letâ€™s say we have a binary classification problem. If we set k to 10, the KNN modell will look for 10 nearest points to the data presented. If among the 10 neighbors observed, 8 of them have the label 0 and 2 of them are labeled 1, the KNN algorithm will conclude that the label of the provided data is most likely also going to be 0. As we can see, the KNN algorithm is extremely simple, but if we have enough data to feed it, it can produce some highly accurate predictions.\n",
    "\n",
    "It can also be used for regression, for example we can take the k nearest neighbors and take the average of their values to predict the value of the data presented for a particular feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_972/3349027751.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance metrics\n",
    "\n",
    "We need to define a metric that tells us how similar or different are two data points. One of the most popular distance metrics is the Euclidean distance. It is defined as the square root of the sum of the squared differences between the two vectors. For example, if we have two vectors $x$ and $y$, the Euclidean distance between them is defined as:\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of features in the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.3290173915275787\n",
      "1.9494646655653247\n",
      "1.5591439385540549\n",
      "0.5356280721938492\n",
      "4.850940186986411\n",
      "2.592833759950511\n",
      "4.214227042632867\n",
      "6.522409988228337\n",
      "4.985585382449795\n"
     ]
    }
   ],
   "source": [
    "def distance(instance1, instance2):\n",
    "    instance1, instance2 = np.array(instance1), np.array(instance2)\n",
    "    return np.sqrt(sum((instance1 - instance2)**2))\n",
    "\n",
    "dataset = [[2.7810836,2.550537003],\n",
    "           [1.465489372,2.362125076],\n",
    "           [3.396561688,4.400293529],\n",
    "           [1.38807019,1.850220317],\n",
    "           [3.06407232,3.005305973],\n",
    "           [7.627531214,2.759262235],\n",
    "           [5.332441248,2.088626775],\n",
    "           [6.922596716,1.77106367],\n",
    "           [8.675418651,-0.242068655],\n",
    "           [7.673756466,3.508563011]]\n",
    "\n",
    "label = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "assert len(dataset) == len(label)\n",
    "\n",
    "for data in dataset:\n",
    "    print(distance(dataset[0], data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbors selection\n",
    "\n",
    "Next, we need to select the neighbors. Given an instance, we select the top k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(training_set, test_instance, k):\n",
    "    distances = [(i, distance(test_instance, instance)) for i, instance in enumerate(training_set)]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return [i[0] for i in distances[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 1, 3, 2, 6, 7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(dataset, dataset[0], k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "To make a prediction, we take the top k neighbors for an instance and then we can use majority voting to select the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(neighbor_index, label):\n",
    "    label = np.array(label)\n",
    "    neighbor_label = label[neighbor_index]\n",
    "    prediction = {}\n",
    "    for x in neighbor_label:\n",
    "        if x in prediction:\n",
    "            prediction[x] += 1\n",
    "        else:\n",
    "            prediction[x] = 1\n",
    "    total = sum(prediction.values())\n",
    "    probability_prediction = {k: v/total for k, v in prediction.items()}\n",
    "    return probability_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7142857142857143, 1: 0.2857142857142857}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction([0, 4, 1, 3, 2, 6, 7], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(training_set, label, test_set, k):\n",
    "    result = []\n",
    "    for instance in test_set:\n",
    "        neighbor_index = get_neighbors(training_set, instance, k)\n",
    "        prediction = make_prediction(neighbor_index, label)\n",
    "        result.append(max(prediction, key=prediction.get))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier(dataset[1:], label, [dataset[0]], 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazy-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
