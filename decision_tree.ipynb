{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Given the whole dataset:\n",
    "\n",
    "1. Calculate the information gain with each possible split (which feature to split on and on which threshold value)\n",
    "2. Divide the set with the feature and the threshold that gives the most information gain\n",
    "3. Repeat for all subbranches, until a stopping criteria is reached\n",
    "\n",
    "# Inferencing\n",
    "\n",
    "1. Traverse the tree with the features of the data point until a leaf is reached\n",
    "2. Return the label of the leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Information gain\n",
    "\n",
    "Information gain is defined as $$IG(node) = Entropy(parent) - [\\text{weighted average}]entropy(children)$$\n",
    "\n",
    "$$Entropy(node) = -\\sum_{i=1}^{c}p_i\\log_2p_i$$\n",
    "$$p_i = \\frac{N_i}{N}$$\n",
    "\n",
    "## Stopping criteria\n",
    "\n",
    "1. Maximum depth: when the tree reaches a maximum depth\n",
    "2. Minimum number of samples per node: when the number of samples in a node falls below a threshold\n",
    "3. Min impurity decrease: when a split results in a decrease of the impurity greater than or equal to this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "\n",
    "    def __init__(self, feature_idx, threshold, left_child=None, right_child=None, *, leaf_value=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.leaf_value = leaf_value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.leaf_value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, min_samples_leaf=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y, level=0)\n",
    "\n",
    "    def _grow_tree(self, X, y, level):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # If we have reached a stopping criteria, return a leaf node\n",
    "        if (level == self.max_depth or n_labels == 1 or n_samples < self.min_samples_leaf):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return TreeNode(None, None, leaf_value=leaf_value)\n",
    "\n",
    "        # Select random features to consider (we don't have to in the case of a Decision Tree, but it's mandatory in the case of a Random Forest)\n",
    "        feature_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
    "\n",
    "        # Greedily select the best split according to information gain\n",
    "        best_feature_idx, best_threshold = self._best_split_criteria(X, y, feature_idxs)\n",
    "        # Split the data using the threshold\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature_idx], best_threshold)\n",
    "\n",
    "        # Assign data points to the left or right child\n",
    "        left_child = self._grow_tree(X[left_idxs, :], y[left_idxs], level+1)\n",
    "        right_child = self._grow_tree(X[right_idxs, :], y[right_idxs], level+1)\n",
    "        return TreeNode(best_feature_idx, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _best_split_criteria(self, X, y, feature_idxs):\n",
    "        best_gain = -1\n",
    "        split_criteria = None\n",
    "        # Iterate through all possible splits and select the pair (feature, threshold) that maximizes information gain\n",
    "        for feature_idx in feature_idxs:\n",
    "            feature = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_criteria = (feature_idx, threshold)\n",
    "        return split_criteria\n",
    "    \n",
    "    def _information_gain(self, y, feature, threshold):\n",
    "        # Calculate the entropy of the parent node\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Calculate the entropy of the left and right child nodes\n",
    "        left_idxs, right_idxs = self._split(feature, threshold)\n",
    "        left_entropy = self._entropy(y[left_idxs])\n",
    "        right_entropy = self._entropy(y[right_idxs])\n",
    "        \n",
    "        # Calculate the information gain\n",
    "        information_gain = parent_entropy - (len(left_idxs)/len(y))*left_entropy - (len(right_idxs)/len(y))*right_entropy\n",
    "        return information_gain\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        # Calculate the entropy of a node according to the formula -sum(p_i*log(p_i)) where p_i is the probability of the i-th class\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = sum(probabilities * -np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        # The index of the data points that are less than or equal to the threshold\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        # The index of the data points that are greater than the threshold\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        # Return the most common label in the data\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x) for x in X])\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Traverse the tree to find the leaf node that the data point belongs to\n",
    "        node = self.root\n",
    "        while not node.is_leaf_node():\n",
    "            if x[node.feature_idx] <= node.threshold:\n",
    "                node = node.left_child\n",
    "            else:\n",
    "                node = node.right_child\n",
    "        return node.leaf_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_split():\n",
    "    MAGIC = 42\n",
    "    random.seed(MAGIC)\n",
    "    np.random.seed(MAGIC)\n",
    "    data = datasets.load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=MAGIC\n",
    "    )\n",
    "\n",
    "    return data, X_train, X_test, y_train, y_test\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (455, 30)\n",
      "Single tree accuracy: 0.9035087719298246\n",
      "mean concave points\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "data, X_train, X_test, y_train, y_test = get_data_split()\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "\n",
    "tree_settings = {\n",
    "    'max_depth': 2,\n",
    "}\n",
    "\n",
    "tree = DecisionTree(**tree_settings)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_predictions = tree.predict(X_test)\n",
    "\n",
    "tree_acc = accuracy(y_test, tree_predictions)\n",
    "print(f'Single tree accuracy: {tree_acc}')\n",
    "\n",
    "# Let's check which feature is the most important one\n",
    "feature_names = data.feature_names\n",
    "print(feature_names[tree.root.feature_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "A random forest is a model that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Each tree is trained on a random subset of the training data. The random subsets are the same size as the original training set, but are created by sampling with replacement. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, n_trees=10, min_samples_leaf=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.n_trees = n_trees\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.trees = []\n",
    "        tree_iterator = tqdm(range(self.n_trees), desc='Training trees')\n",
    "        for _ in tree_iterator:\n",
    "            tree = DecisionTree(min_samples_leaf=self.min_samples_leaf, max_depth=self.max_depth, n_features=self.n_features)\n",
    "            X_sample, Y_sample = self._sample(X, Y)\n",
    "            tree.fit(X_sample, Y_sample)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def _sample(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], Y[indices]\n",
    "    \n",
    "    def _most_common(self, Y):\n",
    "        return np.bincount(Y).argmax()\n",
    "    \n",
    "    def predict(self, X, agg_type='mean'):\n",
    "        predictions = []\n",
    "        tree_iterator = tqdm(self.trees, desc='Predicting')\n",
    "        for tree in tree_iterator:\n",
    "            predictions.append(tree.predict(X))\n",
    "        predictions = np.array(predictions)\n",
    "        if agg_type == 'mean':\n",
    "            return np.mean(predictions, axis=0)\n",
    "        elif agg_type == 'majority':\n",
    "            return np.apply_along_axis(self._most_common, axis=0, arr=predictions)\n",
    "        else:\n",
    "            raise ValueError('agg_type must be either mean or majority')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (455, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training trees: 100%|██████████| 10/10 [00:09<00:00,  1.10it/s]\n",
      "Predicting: 100%|██████████| 10/10 [00:00<00:00, 18774.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.956140350877193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data, X_train, X_test, y_train, y_test = get_data_split()\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "\n",
    "forest = RandomForest(n_trees=10, **tree_settings)\n",
    "forest.fit(X_train, y_train)\n",
    "forest_predictions = forest.predict(X_test, agg_type='majority')\n",
    "\n",
    "tree_acc = accuracy(y_test, forest_predictions)\n",
    "print(f'Random forest accuracy: {tree_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boosting\n",
    "\n",
    "AdaBoosting (which stands for Adaptive Boosting) is an algorithm that allows to combine multiple weak classifiers into a weighted sum to produce a boosted classifier. It is usually used for binary classification, but can be extended to multi-class classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "        self.p = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        # Initially all the samples are classified as 1\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.p == 1:\n",
    "            predictions[X_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X_column > self.threshold] = -1\n",
    "        return predictions\n",
    "    \n",
    "class AdaBoost:\n",
    "\n",
    "    def __init__(self, num_stumps=5):\n",
    "        self.num_stumps = num_stumps\n",
    "        self.stumps = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initial weights are 1 / n_samples\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        for _ in range(self.num_stumps):\n",
    "            stump = Stump()\n",
    "            min_error = float('inf')\n",
    "\n",
    "            # Iterate through every feature and threshold to find the best decision stump\n",
    "            for feature_idx in range(n_features):\n",
    "                X_column = X[:, feature_idx]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    p = 1\n",
    "                    prediction = np.ones(n_samples)\n",
    "                    prediction[X_column < threshold] = -1\n",
    "\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    error = sum(w[y != prediction])\n",
    "\n",
    "                    # If error is over 50% we flip the prediction so that error will be 1 - error\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # Store the best configuration\n",
    "                    if error < min_error:\n",
    "                        stump.feature_idx = feature_idx\n",
    "                        stump.threshold = threshold\n",
    "                        min_error = error\n",
    "                        stump.p = p\n",
    "\n",
    "            # Calculate alpha\n",
    "            EPS = 1e-10\n",
    "            stump.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # Calculate predictions and update weights\n",
    "            predictions = stump.predict(X)\n",
    "\n",
    "            # Only misclassified samples have non-zero weights\n",
    "            w *= np.exp(-stump.alpha * y * predictions)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save stump\n",
    "            self.stumps.append(stump)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = [stump.alpha * stump.predict(X) for stump in self.stumps]\n",
    "        preds = np.sum(preds, axis=0)\n",
    "        preds = np.sign(preds)\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (455, 30)\n",
      "Adaboost accuracy: 0.6052631578947368\n"
     ]
    }
   ],
   "source": [
    "data, X_train, X_test, y_train, y_test = get_data_split()\n",
    "\n",
    "# Change labels to be 1 and -1\n",
    "y_train[y_train == 0] = -1\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "\n",
    "adaboost = AdaBoost(num_stumps = 5)\n",
    "adaboost.fit(X_train, y_train)\n",
    "adaboost_predictions = adaboost.predict(X_test)\n",
    "\n",
    "adaboost_acc = accuracy(y_test, adaboost_predictions)\n",
    "print(f'Adaboost accuracy: {adaboost_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazy-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
